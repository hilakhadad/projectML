{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"oPmZzfftIbIo"},"outputs":[],"source":["import sys\n","import numpy\n","numpy.set_printoptions(threshold=sys.maxsize)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MX2x0D-NOjiY"},"outputs":[],"source":["import pandas as pd\n","from sklearn.feature_selection import RFE\n","from sklearn.svm import LinearSVC\n","import random\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import LinearSVC\n","import numpy as np\n","from sklearn.metrics import matthews_corrcoef, roc_curve\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import average_precision_score\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iatYnWaD0jO4"},"outputs":[],"source":["from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rR68qsGG0hRy"},"outputs":[],"source":["from sklearn.feature_selection import SelectFdr\n","from sklearn.feature_selection import f_classif\n","from sklearn.feature_selection import SelectKBest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NVieBz_B04q4"},"outputs":[],"source":["def set_random_seed(seed_value):\n","    np.random.seed(seed_value)\n","    random.seed(seed_value)\n","    \n","set_random_seed(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K63WE9ksx13B"},"outputs":[],"source":["def init_classifiers():\n","  gnb = GaussianNB()\n","  svc = SVC(probability=True) \n","  lr = LogisticRegression()\n","  rf = RandomForestClassifier()\n","  knn = KNeighborsClassifier()\n","  return [gnb, svc, lr, rf, knn]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CTKxyX21CmSr"},"outputs":[],"source":["def splits(num_of_records):\n","  if num_of_records<50:\n","    return split_LPO\n","  elif num_of_records<100:\n","    return split_LOO\n","  elif num_of_records>1000:\n","    return split_5Fold\n","  else:\n","    return split_10Fold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wE5-WZQX-eX_"},"outputs":[],"source":["from sklearn.model_selection import LeaveOneOut\n","\n","def split_LOO(X,y):\n","  Xs_train, Xs_test, ys_train, ys_test = [],[],[],[]\n","  loo = LeaveOneOut()\n","  folds = loo.get_n_splits(X)\n","\n","  for train_index, test_index in loo.split(X):\n","      X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","      y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","      Xs_train.append(X_train)\n","      Xs_test.append(X_test)\n","      ys_train.append(y_train)\n","      ys_test.append(y_test)\n","\n","  return Xs_train, Xs_test, ys_train, ys_test, folds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eBlSPKE0QsSU"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.decomposition import PCA\n","\n","def pca_scores(X):\n","  pca = PCA(n_components=1)\n","  pca.fit(X)\n","  components = abs(pca.components_[0])\n","  selected_features =[x for _,x in sorted(zip(components, range(1, len(components) + 1)), reverse=True)]\n","  return selected_features\n","  \n","def DTScore(X, y):\n","  clf = DecisionTreeClassifier(random_state=42)\n","  clf = clf.fit(X, y)\n","  \n","  selected_features =[x for _,x in sorted(zip(clf.feature_importances_, range(1, len(clf.feature_importances_) + 1)), reverse=True)]\n","  return selected_features\n","\n","def algo2(X, y):\n","  rank_1 = pca_scores(X)\n","  rank_2 = DTScore(X, y)\n","  interleaved_ranking = [val for pair in zip(rank_2, rank_1) for val in pair]\n","  ranking_no_duplicates = list(dict.fromkeys(interleaved_ranking))\n","  ranking_array_features_places = np.zeros(X.shape[1])\n","  j = 1\n","  for i in range(X.shape[1]):\n","    ranking_of_i_best_feature = ranking_no_duplicates[i] - 1\n","    ranking_array_features_places[ranking_of_i_best_feature] = j\n","    j += 1\n","  return 1 / ranking_array_features_places\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"amVxmmrO_xgA"},"outputs":[],"source":["from sklearn.model_selection import StratifiedKFold\n","\n","def split_5Fold(X,y):\n","  Xs_train, Xs_test, ys_train, ys_test = [],[],[],[]\n","  kf = StratifiedKFold(n_splits=5)\n","  folds = kf.get_n_splits(X)\n","\n","  for train_index, test_index in kf.split(X,y):\n","    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","    Xs_train.append(X_train)\n","    Xs_test.append(X_test)\n","    ys_train.append(y_train)\n","    ys_test.append(y_test)\n","\n","  return Xs_train, Xs_test, ys_train, ys_test, folds\n","\n","def split_10Fold(X,y):\n","  Xs_train, Xs_test, ys_train, ys_test = [],[],[],[]\n","  kf = StratifiedKFold(n_splits=10)\n","  folds = kf.get_n_splits(X)\n","\n","  for train_index, test_index in kf.split(X,y):\n","    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","    Xs_train.append(X_train)\n","    Xs_test.append(X_test)\n","    ys_train.append(y_train)\n","    ys_test.append(y_test)\n","\n","  return Xs_train, Xs_test, ys_train, ys_test, folds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y7efGuk1AqS7"},"outputs":[],"source":["from sklearn.model_selection import LeavePOut\n","\n","def split_LPO(X,y):\n","  Xs_train, Xs_test, ys_train, ys_test = [],[],[],[]\n","  lpo = LeavePOut(2)\n","  folds = lpo.get_n_splits(X)\n","\n","  for train_index, test_index in lpo.split(X):\n","    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","    Xs_train.append(X_train)\n","    Xs_test.append(X_test)\n","    ys_train.append(y_train)\n","    ys_test.append(y_test)\n","\n","  return Xs_train, Xs_test, ys_train, ys_test, folds"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Db1JfTzOha4d"},"outputs":[],"source":["def get_dataset_files():\n","  return [\"clean/CLL.csv\",\n","          \"clean/COPDSexualDimorphism.data.csv\",\n","          \"clean/DLBCL.csv\",\n","          \"clean/Leukemia_3c_arff.csv\",\n","          \"clean/Leukemia_4c_arff.csv\",\n","          \"clean/Prostate.csv\",\n","          \"clean/breastCancerVDX.csv\",\n","          \"clean/breast_arff.csv\",\n","          \"clean/colon.csv\",\n","          \"clean/curatedOvarianData.csv\",\n","          \"clean/leukemia.csv\",\n","          \"clean/lung.csv\",\n","          \"clean/lung_arff.csv\",\n","          \"clean/lymphoma.csv\",\n","          \"clean/lymphoma_arff.csv\",\n","          \"clean/misc1.csv\",\n","          \"clean/misc2.csv\",\n","          \"clean/misc3.csv\",\n","          \"clean/misc4.csv\",\n","          \"clean/misc5.csv\"\n","      ]\n","\n","def parse_file(dataset_file):\n","  name = dataset_file.split('/')[-1].split('.')[0]\n","  print(f'Starting to work on dataset: {name}')\n","  df = pd.read_csv(dataset_file)\n","  X = df.drop(columns=['target'])\n","  y = df['target']\n","  print(f'X shape: {X.shape}')\n","  return name, df, X, y\n","\n","def get_splits(X, y):\n","  num_of_samples = X.shape[0]\n","  splitter = splits(num_of_samples)\n","  cv_type_name = splitter.__name__\n","  print(f'CV type: {cv_type_name}')\n","  X_trains, X_tests, y_trains, y_tests, n_of_folds = splitter(X,y)\n","  print(f'Number of CV folds: {n_of_folds}')\n","  return X_trains, X_tests, y_trains, y_tests, n_of_folds, cv_type_name\n","\n","def get_split(X_trains, X_tests, y_trains, y_tests, fold_i):\n","  X_train = X_trains[fold_i]\n","  X_test = X_tests[fold_i]\n","  y_train = y_trains[fold_i]\n","  y_test = y_tests[fold_i]\n","  return X_train, X_test, y_train, y_test\n","\n","def calculate_feature_rankings_and_wrap_for_kbest(X, y):\n","  algo2_ranked = algo2(X, y)\n","  def kbestwrapper(X, y):\n","    return algo2_ranked\n","  return kbestwrapper\n","\n","def create_data_dict_empty():\n","  result_cols = [\"Dataset Name\",\"Number of samples\",\"Original Number of features\",\"Filtering Algorithm\",\"Learning algorithm\",\"Number of features selected (K)\",\"CV Method\",\n","                \"Fold\",\"Measure Type\",\"Measure Value\",\"List of Selected Features Names\",\"Selected Features scores\"]\n","  data_dict = {}\n","  for column in result_cols:\n","    data_dict[column] = []\n","  return data_dict\n","\n","def fill_row(data_dict, *args):\n","  result_cols = [\"Dataset Name\",\"Number of samples\",\"Original Number of features\",\"Filtering Algorithm\",\"Learning algorithm\",\"Number of features selected (K)\",\"CV Method\",\n","                \"Fold\",\"Measure Type\",\"Measure Value\",\"List of Selected Features Names\",\"Selected Features scores\"]\n","  i = 0\n","  for arg in args:\n","    data_dict[result_cols[i]].append(arg)\n","    i += 1\n","\n","def format_e(num):\n","    return format(num,'.2E')\n","format_e2 = np.vectorize(format_e)\n","\n","# Change with each notebook\n","FSMethod_name = \"algo2\"\n","result_table_dict = create_data_dict_empty()\n","for dataset_file in get_dataset_files():\n","  dataset_name, df, X, y = parse_file(dataset_file)\n","  n_samples, n_orig_features = X.shape\n","  X_trains, X_tests, y_trains, y_tests, n_of_folds, cv_type_name = get_splits(X, y)\n","  j = 0\n","  for fold_i in range(n_of_folds):\n","    print(f'Fold {fold_i + 1}:')\n","    X_train, X_test, y_train, y_test = get_split(X_trains, X_tests, y_trains, y_tests, fold_i)\n","    start = time.time()\n","    kbestwrapper = calculate_feature_rankings_and_wrap_for_kbest(X_train, y_train)\n","    end = time.time()\n","    FSMethod_time = end - start\n","    FSMethod_time = format(FSMethod_time,'.2E')\n","    print(f'Time (seconds) took for {FSMethod_name}: {FSMethod_time}')\n","    for k in [1,2,3,4,5,10,15,20,25,30,50,100][::-1]:\n","      print(f'k {k}:')\n","      select_k_best = SelectKBest(kbestwrapper, k=k).fit(X_train, y_train)\n","      k_feature_names = select_k_best.get_feature_names_out()\n","      k_feature_scores = select_k_best.scores_[select_k_best.get_support()]\n","      k_feature_scores = format_e2(k_feature_scores)\n","      print(k_feature_scores)\n","      X_train_reduced = select_k_best.transform(X_train)\n","      print(X_train_reduced.shape)\n","      X_test_reduced = select_k_best.transform(X_test)\n","      print(X_test_reduced.shape)\n","      for clf in init_classifiers():\n","        j += 1\n","        start = time.time()\n","        clf.fit(X_train_reduced, y_train)\n","        end = time.time()\n","        clf_fit_time = end - start\n","        clf_name = clf.__class__.__name__\n","        clf_fit_time = format(clf_fit_time,'.2E')\n","        print(f'Time (seconds) took to fit {clf_name}: {clf_fit_time}')\n","\n","        start = time.time()\n","        y_probas = clf.predict_proba(X_test_reduced)\n","        end = time.time()\n","        clf_inference_time_per_record = (end - start) / X_test_reduced.shape[0]\n","        clf_inference_time_per_record = format(clf_inference_time_per_record,'.2E')\n","        print(f'Time (seconds) took for inference per record: {clf_inference_time_per_record}')\n","\n","        acc = clf.score(X_test_reduced, y_test)\n","        mcc = matthews_corrcoef(clf.predict(X_test_reduced), y_test)\n","\n","        try:\n","          if len(y.unique()) > 2:\n","            auc = roc_auc_score(y_test, y_probas, multi_class = 'ovr')\n","          else:\n","            auc = roc_auc_score(y_test, y_probas[:,1])\n","        except:\n","          auc = np.nan\n","        \n","        try:\n","          pr_auc = average_precision_score(y_test, y_probas[:, 1])\n","        except:\n","          pr_auc = np.nan\n","        \n","        acc = format(acc,'.2E')\n","        mcc = format(mcc,'.2E')\n","        auc = format(auc,'.2E')\n","        pr_auc = format(pr_auc,'.2E')\n","\n","        print(f'{j} accuracy: {acc}')\n","        print(f'{j} mcc: {mcc}')\n","        print(f'{j} auc: {auc}')\n","        print(f'{j} pr_auc: {pr_auc}')\n","        fold_iter = fold_i + 1\n","        fill_row(result_table_dict, dataset_name, n_samples, n_orig_features, FSMethod_name, clf_name, k, cv_type_name, fold_iter, \"ACC\", acc, k_feature_names, k_feature_scores)\n","        fill_row(result_table_dict, dataset_name, n_samples, n_orig_features, FSMethod_name, clf_name, k, cv_type_name, fold_iter, \"MCC\", mcc, k_feature_names, k_feature_scores)\n","        fill_row(result_table_dict, dataset_name, n_samples, n_orig_features, FSMethod_name, clf_name, k, cv_type_name, fold_iter, \"AUC\", auc, k_feature_names, k_feature_scores)\n","        fill_row(result_table_dict, dataset_name, n_samples, n_orig_features, FSMethod_name, clf_name, k, cv_type_name, fold_iter, \"PR-AUC\", pr_auc, k_feature_names, k_feature_scores)\n","        fill_row(result_table_dict, dataset_name, n_samples, n_orig_features, FSMethod_name, clf_name, k, cv_type_name, fold_iter, \"FSMethod time(seconds)\", FSMethod_time, k_feature_names, k_feature_scores)\n","        fill_row(result_table_dict, dataset_name, n_samples, n_orig_features, FSMethod_name, clf_name, k, cv_type_name, fold_iter, \"CLF fit time(seconds)\", clf_fit_time, k_feature_names, k_feature_scores)\n","        fill_row(result_table_dict, dataset_name, n_samples, n_orig_features, FSMethod_name, clf_name, k, cv_type_name, fold_iter, \"CLF inference time per record(seconds)\", clf_inference_time_per_record, k_feature_names, k_feature_scores)\n","     \n","result_table_df = pd.DataFrame(result_table_dict)\n","result_table_df.to_csv(f'clean/results_table_{FSMethod_name}.csv')"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"runAlgo2.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}